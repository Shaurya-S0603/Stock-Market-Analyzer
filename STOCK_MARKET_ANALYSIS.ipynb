{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shaurya-S0603/Stock-Market-Analyzer/blob/main/STOCK_MARKET_ANALYSIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tgn-n-ORg1p0"
      },
      "source": [
        "# **CREATING THE BOT AND TRAINING IT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PuSUv5Gb3i-"
      },
      "source": [
        "**Importing Required Packages.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Z37gBmkjbvyb"
      },
      "outputs": [],
      "source": [
        "!pip install yfinance ta xgboost lightgbm catboost pandas_datareader requests textblob --quiet\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas_datareader.data as web\n",
        "from textblob import TextBlob\n",
        "from ta import add_all_ta_features\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fetching data from drive**"
      ],
      "metadata": {
        "id": "CUXCASmrt-iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/StockMarketAnalyzer/')\n",
        "\n",
        "CACHE_FILENAME ='/content/drive/StockMarketAnalyzer/stock_sentiment_cache.csv'\n",
        "\n",
        "from google.colab import userdata\n",
        "fmp = userdata.get('fmp')\n",
        "fsa = userdata.get('fsa')"
      ],
      "metadata": {
        "id": "NyA7GfKauB8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JQxn9-3b9Ff"
      },
      "source": [
        "**Getting the training dataset and exploring it**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXros3zCb80P",
        "outputId": "dd1f90c0-5ea7-4210-d93a-8b4b006731c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "# Replace 'stock_symbol' with the stock symbol you want to analyze\n",
        "stock_symbol = 'GOOG'\n",
        "start_date = '2021-01-01'\n",
        "end_date = pd.Timestamp.now().strftime('%Y-%m-%d')\n",
        "\n",
        "FMP_API_KEY = fmp\n",
        "FINNHUB_API_KEY = fsa\n",
        "\n",
        "stock_data = yf.download(stock_symbol, start=start_date, end=end_date)\n",
        "if isinstance(stock_data.columns, pd.MultiIndex):\n",
        "    stock_data.columns = stock_data.columns.droplevel(1)\n",
        "stock_data = stock_data.interpolate(method='time').bfill().ffill().copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FMP,AV,FSA,NDL Data**"
      ],
      "metadata": {
        "id": "tJ415cqeqbtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_fundamental_data(symbol, apikey):\n",
        "    url = f'https://financialmodelingprep.com/api/v3/income-statement/{symbol}?limit=1&apikey={apikey}'\n",
        "    resp = requests.get(url)\n",
        "    data = resp.json()[0] if resp.status_code == 200 and isinstance(resp.json(), list) and len(resp.json()) else {}\n",
        "    return {\n",
        "        \"eps\": data.get(\"eps\", np.nan),\n",
        "        \"grossProfit\": data.get(\"grossProfit\", np.nan),\n",
        "        \"revenue\": data.get(\"revenue\", np.nan),\n",
        "        \"costOfRevenue\": data.get(\"costOfRevenue\", np.nan),\n",
        "    }\n",
        "\n",
        "fundamentals = fetch_fundamental_data(stock_symbol, FMP_API_KEY)\n",
        "for k, v in fundamentals.items():\n",
        "    stock_data[k] = v\n",
        "\n",
        "try:\n",
        "    cpi = web.DataReader('CPIAUCSL', 'fred', start_date, end_date)\n",
        "    cpi = cpi.reindex(stock_data.index).ffill().bfill()\n",
        "    stock_data['cpius'] = cpi.squeeze().values\n",
        "except Exception as e:\n",
        "    print(f\"Error fetching macro data: {e}\")\n",
        "    stock_data['cpius'] = np.nan\n",
        "\n",
        "def fetch_news_headlines(symbol, date_str, apikey):\n",
        "    url = f'https://finnhub.io/api/v1/company-news?symbol={symbol}&from={date_str}&to={date_str}&token={apikey}'\n",
        "    try:\n",
        "        resp = requests.get(url)\n",
        "        news = resp.json()\n",
        "        return [article['headline'] for article in news if 'headline' in article]\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def compute_sentiment(headlines):\n",
        "    if not headlines:\n",
        "        return 0.0\n",
        "    scores = [TextBlob(headline).sentiment.polarity for headline in headlines]\n",
        "    return np.mean(scores)\n",
        "\n",
        "def build_sentiment_index(symbol, dates, apikey):\n",
        "    sentiment_vals = []\n",
        "    for date in dates:\n",
        "        date_str = date.strftime('%Y-%m-%d')\n",
        "        headlines = fetch_news_headlines(symbol, date_str, apikey)\n",
        "        sentiment_score = compute_sentiment(headlines)\n",
        "        sentiment_vals.append(sentiment_score)\n",
        "    return pd.Series(sentiment_vals, index=dates)\n",
        "\n",
        "def load_or_build_sentiment(symbol, dates, apikey, cache_file):\n",
        "    if os.path.exists(cache_file):\n",
        "        print(f\"Loading cached sentiment data from {cache_file} ...\")\n",
        "        cached = pd.read_csv(cache_file, index_col=0, parse_dates=True)\n",
        "        missing_dates = dates.difference(cached.index)\n",
        "        if len(missing_dates) == 0:\n",
        "            return cached['sentiment']\n",
        "        else:\n",
        "            print(f\"Fetching sentiment for missing {len(missing_dates)} dates...\")\n",
        "            new_sentiments = build_sentiment_index(symbol, missing_dates, apikey)\n",
        "            combined = pd.concat([cached['sentiment'], new_sentiments])\n",
        "            combined = combined.sort_index()\n",
        "            combined.to_csv(cache_file)\n",
        "            return combined\n",
        "    else:\n",
        "        print(\"Cache not found, fetching sentiment for all dates...\")\n",
        "        sentiments = build_sentiment_index(symbol, dates, apikey)\n",
        "        sentiments.to_csv(cache_file)\n",
        "        return sentiments\n",
        "\n",
        "print(\"Fetching historical news sentiment, this may take a few minutes depending on date range...\")\n",
        "hist_sentiment = load_or_build_sentiment(stock_symbol, stock_data.index, FINNHUB_API_KEY, CACHE_FILENAME)\n",
        "stock_data['news_sentiment'] = hist_sentiment.reindex(stock_data.index).fillna(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IALqYZ6pqbd7",
        "outputId": "b96fc0f9-52f6-4631-adcd-d254e1ece062"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching historical news sentiment, this may take a few minutes depending on date range...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Engineering**"
      ],
      "metadata": {
        "id": "_XKYEkpWkoor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def engineer_features(df):\n",
        "    data = df.copy()\n",
        "    original_index = data.index\n",
        "    data = data.reset_index()\n",
        "\n",
        "    for col in ['Open', 'High', 'Low', 'Close', 'Volume']:\n",
        "        if col in data.columns:\n",
        "            if isinstance(data[col], pd.DataFrame):\n",
        "                data[col] = data[col].iloc[:, 0]\n",
        "            else:\n",
        "                data[col] = data[col].squeeze()\n",
        "\n",
        "    data = add_all_ta_features(\n",
        "        data, open=\"Open\", high=\"High\", low=\"Low\", close=\"Close\", volume=\"Volume\", fillna=True\n",
        "    )\n",
        "    data = data.set_index(original_index)\n",
        "\n",
        "    for lag in range(1, 6):\n",
        "        data[f'Close_Lag_{lag}'] = data['Close'].shift(lag)\n",
        "    data['Volatility_21'] = data['Close'].rolling(window=21).std()\n",
        "\n",
        "    if not data.dropna().empty:\n",
        "        data.dropna(inplace=True)\n",
        "    else:\n",
        "        print(\"Warning: Dropping NaNs resulted in an empty DataFrame. Consider adjusting the date range or feature engineering steps.\")\n",
        "\n",
        "    return data\n",
        "\n",
        "engineered_data = engineer_features(stock_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGEVWbzAkoMH",
        "outputId": "3fac8a53-b1ef-4169-9429-dbfaeed51cb4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Dropping NaNs resulted in an empty DataFrame. Consider adjusting the date range or feature engineering steps.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hErsQ5uOdjtO"
      },
      "source": [
        "**Implementing the SMA (Simple Moving Avg) Stratergy**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "07joEmaueqy1"
      },
      "outputs": [],
      "source": [
        "def SMA_strategy(data, short_window=20, long_window=50):\n",
        "    signals = pd.DataFrame(index=data.index)\n",
        "    signals['signal'] = 0.0\n",
        "    signals['short_mavg'] = data['Close'].rolling(window=short_window, min_periods=1).mean()\n",
        "    signals['long_mavg'] = data['Close'].rolling(window=long_window, min_periods=1).mean()\n",
        "    signals.loc[signals.index[short_window:], 'signal'] = np.where(\n",
        "        signals['short_mavg'].iloc[short_window:] > signals['long_mavg'].iloc[short_window:], 1.0, 0.0\n",
        "    )\n",
        "    signals['positions'] = signals['signal'].diff()\n",
        "    return signals\n",
        "\n",
        "signals = SMA_strategy(engineered_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PREDICTION**"
      ],
      "metadata": {
        "id": "nQtFlUp3lBIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = engineered_data.drop(columns=['Close', 'Date'], errors='ignore')\n",
        "y = engineered_data['Close']\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "xgb_params = {'n_estimators': [2000, 5000], 'learning_rate': [0.001, 0.002]}\n",
        "model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "grid = GridSearchCV(model, param_grid=xgb_params, cv=tscv, scoring='neg_mean_squared_error')\n",
        "grid.fit(X, y)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "split_idx = int(len(X) * 0.8)\n",
        "X_train, X_test = X.iloc[:split_idx].copy(), X.iloc[split_idx:].copy()\n",
        "y_train, y_test = y.iloc[:split_idx].copy(), y.iloc[split_idx:].copy()\n",
        "\n",
        "best_model.fit(X_train, y_train)\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "test_mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(test_mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "residuals = y_test - y_pred\n",
        "error_std = np.std(residuals)\n",
        "\n",
        "future_days = 5\n",
        "future_dates = pd.bdate_range(start=engineered_data.index[-1] + pd.Timedelta(days=1), periods=future_days)\n",
        "last_known_features = X.iloc[-1].copy()\n",
        "future_predictions = []\n",
        "last_close = y.iloc[-1]\n",
        "\n",
        "for i in range(future_days):\n",
        "    pred_close = best_model.predict(last_known_features.values.reshape(1, -1))[0]\n",
        "    future_predictions.append(pred_close)\n",
        "    for lag in range(5, 1, -1):\n",
        "        last_known_features[f'Close_Lag_{lag}'] = last_known_features[f'Close_Lag_{lag-1}']\n",
        "    last_known_features['Close_Lag_1'] = pred_close\n",
        "    if 'Close' in last_known_features.index:\n",
        "        last_known_features['Close'] = pred_close\n",
        "\n",
        "future_df = pd.DataFrame({'Date': future_dates, 'Predicted_Close': future_predictions}).set_index('Date')\n",
        "predicted_return = (future_df['Predicted_Close'].iloc[-1] - last_close) / last_close * 100\n",
        "\n",
        "buy_threshold = 1.0\n",
        "sell_threshold = -1.0\n",
        "if predicted_return > buy_threshold:\n",
        "    decision = \"Invest (Buy)\"\n",
        "elif predicted_return < sell_threshold:\n",
        "    decision = \"Pull out (Sell)\"\n",
        "else:\n",
        "    decision = \"Hold / No action\"\n",
        "\n",
        "conf_interval = (predicted_return - 2 * error_std, predicted_return + 2 * error_std)\n"
      ],
      "metadata": {
        "id": "hxa4v-GqafFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results**"
      ],
      "metadata": {
        "id": "cbbgGFDes56r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Investment Decision based on 5-day forecast: {decision}\")\n",
        "print(f\"Predicted 5-day return: {predicted_return:.3f}%\")\n",
        "print(f\"Test MSE: {test_mse:.4f}\")\n",
        "print(f\"Test RMSE: {rmse:.4f}\")\n",
        "print(f\"Test R^2 Score: {r2:.4f}\")\n",
        "print(f\"Approximate 95% confidence interval for predicted return error: [{conf_interval[0]:.3f}%, {conf_interval[1]:.3f}%]\")\n",
        "print(f\"\\n5-Day Forecast:\\n{future_df}\")"
      ],
      "metadata": {
        "id": "CZCgDqGQs80k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot: Actual vs Forecast**"
      ],
      "metadata": {
        "id": "sYuV1UPds97t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(engineered_data.index, engineered_data['Close'], label='Actual Close Price', color='blue')\n",
        "plt.plot(future_df.index, future_df['Predicted_Close'], label='Predicted Close Price', color='red', linestyle='dashed')\n",
        "plt.title(f\"{stock_symbol} Close Price and 5-Day Forecast\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "teRUnJPutDfT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}